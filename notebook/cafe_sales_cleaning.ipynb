{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dd1cc48",
   "metadata": {},
   "source": [
    "# â˜• CafÃ© Sales Data Cleaning\n",
    "\n",
    "Welcome to my notebook!  \n",
    "\n",
    "This project is about **cleaning a messy cafÃ© sales dataset**.  \n",
    "The dataset contains multiple issues such as:  \n",
    "\n",
    "- Missing values in numeric and categorical columns  \n",
    "- Wrong or placeholder values like `ERROR` or `UNKNOWN`  \n",
    "- Inconsistent transaction dates  \n",
    "- Plausibility errors (e.g., Quantity Ã— Price not matching Total Spent)  \n",
    "\n",
    "### Goals of this notebook:\n",
    "1. Load and inspect the raw dataset  \n",
    "2. Identify missing values and inconsistencies  \n",
    "3. Apply systematic **data cleaning steps**  \n",
    "4. Recalculate values when possible  \n",
    "5. Export a clean dataset and a list of invalid rows  \n",
    "6. Summarize fixes in a **cleaning report**  \n",
    "\n",
    "This notebook is written in a style similar to Kaggle kernels: **Markdown explanations alternating with code cells**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66110851",
   "metadata": {},
   "source": [
    "# CafÃ© Sales Data Cleaning\n",
    "\n",
    "In this notebook, I clean and prepare a dataset of cafÃ© sales that contains multiple data quality issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d8df9e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by importing the necessary libraries and adjusting Pandas display settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c2ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 100)    # show up to 100 rows\n",
    "pd.set_option('display.max_columns', None)  # show all columns\n",
    "pd.set_option('display.expand_frame_repr', False)  # no truncation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2949c5e0",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "We read the raw CSV file with error handling to catch common issues like missing files or parsing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c221a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv(\"dirty_cafe_sales.csv\")\n",
    "    print(\"file successfully read\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: file 'dirty_cafe_sales.csv' not found.\")\n",
    "    df = None\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Error: file is empty.\")\n",
    "    df = None\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: file could not be read (parse error).\")\n",
    "    df = None\n",
    "except Exception as e:\n",
    "    print(f\"An unknown error occurred: {e}\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc0301",
   "metadata": {},
   "source": [
    "## First Look at the Data\n",
    "\n",
    "We check the first rows, dataset info, and descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce88038",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5fdd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dabd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8c0dd2",
   "metadata": {},
   "source": [
    "## Random Sample\n",
    "\n",
    "To get a better overview, let's look at 100 random rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa7cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068d20f1",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "We count missing values per column and show all rows that contain NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ac6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count = df.isnull().sum()\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab459e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e462babc",
   "metadata": {},
   "source": [
    "## Data Cleaning Steps\n",
    "\n",
    "We now convert numeric columns, handle missing values, and recalculate when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adefe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"Quantity\", \"Price Per Unit\", \"Total Spent\"]\n",
    "for col in num_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df[num_cols] = df[num_cols].fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f2cde3",
   "metadata": {},
   "source": [
    "### Recalculating numeric fields\n",
    "We recalculate missing values for Price Per Unit, Total Spent, and Quantity where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2de080",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixes = {\"Price Per Unit\": 0, \"Total Spent\": 0, \"Quantity\": 0, \n",
    "         \"Item\": 0, \"Payment Method\": 0, \"Location\": 0, \"Transaction Date\": 0}\n",
    "\n",
    "mask_price_missing = (df[\"Price Per Unit\"] == 0.0) & (df[\"Total Spent\"] > 0) & (df[\"Quantity\"] > 0)\n",
    "fixes[\"Price Per Unit\"] = mask_price_missing.sum()\n",
    "df.loc[mask_price_missing, \"Price Per Unit\"] = df[\"Total Spent\"] / df[\"Quantity\"]\n",
    "\n",
    "mask_total_missing = (df[\"Total Spent\"] == 0.0) & (df[\"Price Per Unit\"] > 0) & (df[\"Quantity\"] > 0)\n",
    "fixes[\"Total Spent\"] = mask_total_missing.sum()\n",
    "df.loc[mask_total_missing, \"Total Spent\"] = df[\"Price Per Unit\"] * df[\"Quantity\"]\n",
    "\n",
    "mask_quantity_missing = (df[\"Quantity\"] == 0.0) & (df[\"Price Per Unit\"] > 0) & (df[\"Total Spent\"] > 0)\n",
    "fixes[\"Quantity\"] = mask_quantity_missing.sum()\n",
    "df.loc[mask_quantity_missing, \"Quantity\"] = df[\"Total Spent\"] / df[\"Price Per Unit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48778a1b",
   "metadata": {},
   "source": [
    "### Cleaning categorical values\n",
    "We clean values for Item, Payment Method, Location, and Transaction Date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9f5eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_items = df[\"Item\"].isin([\"ERROR\", \"UNKNOWN\"]).sum() + df[\"Item\"].isna().sum()\n",
    "\n",
    "df[\"Item\"] = df[\"Item\"].replace([\"ERROR\", \"UNKNOWN\"], pd.NA).fillna(\"Unknown Item\")\n",
    "\n",
    "n_salad = ((df[\"Price Per Unit\"] == 5) & (df[\"Item\"].str.lower() == \"unknown item\")).sum()\n",
    "n_coffee = ((df[\"Price Per Unit\"] == 2) & (df[\"Item\"].str.lower() == \"unknown item\")).sum()\n",
    "n_tea = ((df[\"Price Per Unit\"] == 1.5) & (df[\"Item\"].str.lower() == \"unknown item\")).sum()\n",
    "\n",
    "df.loc[(df[\"Price Per Unit\"] == 5) & (df[\"Item\"].str.lower() == \"unknown item\"), \"Item\"] = \"Salad\"\n",
    "df.loc[(df[\"Price Per Unit\"] == 2) & (df[\"Item\"].str.lower() == \"unknown item\"), \"Item\"] = \"Coffee\"\n",
    "df.loc[(df[\"Price Per Unit\"] == 1.5) & (df[\"Item\"].str.lower() == \"unknown item\"), \"Item\"] = \"Tea\"\n",
    "\n",
    "fixes[\"Item\"] = before_items + n_salad + n_coffee + n_tea\n",
    "\n",
    "before_payment = df[\"Payment Method\"].isin([\"ERROR\", \"UNKNOWN\"]).sum() + df[\"Payment Method\"].isna().sum()\n",
    "fixes[\"Payment Method\"] = before_payment\n",
    "df[\"Payment Method\"] = df[\"Payment Method\"].replace([\"ERROR\", \"UNKNOWN\"], pd.NA).fillna(\"Unknown\")\n",
    "\n",
    "before_location = df[\"Location\"].isin([\"ERROR\", \"UNKNOWN\"]).sum() + df[\"Location\"].isna().sum()\n",
    "fixes[\"Location\"] = before_location\n",
    "df[\"Location\"] = df[\"Location\"].replace([\"ERROR\", \"UNKNOWN\"], pd.NA).fillna(\"Unknown\")\n",
    "\n",
    "before_date = df[\"Transaction Date\"].isna().sum()\n",
    "fixes[\"Transaction Date\"] = before_date\n",
    "df[\"Transaction Date\"] = df[\"Transaction Date\"].replace([\"ERROR\", \"UNKNOWN\"], pd.NA)\n",
    "df[\"Transaction Date\"] = pd.to_datetime(df[\"Transaction Date\"], errors=\"coerce\")\n",
    "\n",
    "df = df.sort_values(by=\"Transaction Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d7608",
   "metadata": {},
   "source": [
    "### Plausibility Check\n",
    "We verify if Quantity Ã— Price Per Unit equals Total Spent and export invalid rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92db2535",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Check_Total_OK\"] = (df[\"Quantity\"] * df[\"Price Per Unit\"]).round(2) == df[\"Total Spent\"].round(2)\n",
    "invalid_rows = df[~df[\"Check_Total_OK\"]]\n",
    "invalid_count = len(invalid_rows)\n",
    "\n",
    "df.to_csv(\"clean_cafe_sales.csv\", index=False)\n",
    "invalid_rows.to_csv(\"transactions_invalid.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3898ca05",
   "metadata": {},
   "source": [
    "## Cleaning Report\n",
    "We summarize how many values were fixed and how many invalid rows remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e01eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in fixes.items():\n",
    "    print(f\"{key} fixed/filled: {val}\")\n",
    "print(f\"Invalid rows (plausibility check failed): {invalid_count}\")\n",
    "print(\"Files saved: clean_cafe_sales.csv, transactions_invalid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0964af",
   "metadata": {},
   "source": [
    "## Inspect Cleaned Data\n",
    "Finally, let's look at some samples from the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff0fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"clean_cafe_sales.csv\")\n",
    "df2.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7172dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52431394",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.iloc[9450:9461]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcbe067",
   "metadata": {},
   "source": [
    "# ðŸ“Š Summary & Next Steps\n",
    "\n",
    "### What we achieved:\n",
    "- Converted and cleaned numeric columns (`Quantity`, `Price Per Unit`, `Total Spent`)  \n",
    "- Replaced or inferred missing values where possible  \n",
    "- Fixed invalid entries in `Item`, `Payment Method`, `Location`, and `Transaction Date`  \n",
    "- Ensured dataset is sorted chronologically  \n",
    "- Added a plausibility check for transactions  \n",
    "- Exported two files:  \n",
    "  - `clean_cafe_sales.csv` â†’ cleaned dataset  \n",
    "  - `transactions_invalid.csv` â†’ invalid rows  \n",
    "\n",
    "### Next steps:\n",
    "- Perform **exploratory data analysis (EDA)** on the cleaned dataset  \n",
    "- Visualize sales trends (by date, item, location, payment method)  \n",
    "- Build a **predictive model** (e.g., forecasting daily sales)  \n",
    "\n",
    "---\n",
    "\n",
    "âœ… With this notebook, recruiters and data teams can see how I approach **real-world messy data**: carefully, systematically, and with reproducible code.  \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
